<html height=100% lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Chenchen Xu - Home Page</title>
    
    <meta name="author" content="Chenchen Xu">
    <!-- <meta name="viewport" content="width=device-width, initial-scale=1"> -->
    
    <!-- <link rel="stylesheet" type="text/css" href="glab.css"> -->
  </head>
  
  <body data-new-gr-c-s-check-loaded="14.1062.0" data-gr-ext-installed="" style="margin: 0;padding: 0;">
    <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">

              <td style="padding:0%;width:85%;vertical-align:middle">
                <p style="text-align:center">
            <name><strong><a href="https://orcid.org/0000-0003-4778-8547">Chenchen Xu</a>  (许晨晨)</strong></name>
          </p><p>I am a PhD student of the <a href="http://www.cad.zju.edu.cn/zhongwen.html">State Key Lab of CAD&CG</a>, <a href="https://www.zju.edu.cn/">Zhejiang University</a>, supervised by Prof. 
              <a href="http://www.cad.zju.edu.cn/home/weiweixu/weiweixu_en.htm">Weiwei Xu</a>. I did my Undergrad and Masters degree at <a href="https://www.ahnu.edu.cn/">Anhui Normal University</a>.
              <br>My research interests are deep learning, layout generation and action recognition. 
              <br><br>
              E-mail: xuchenchen@zju.edu.cn</p>
              </td>
              <td style="padding:2.5%;width:20%;max-width:40%">
                <!-- <img src="images/person.png" width="100px"> -->
                <img src="images/person.png" style="padding-top: 23;padding-bottom: 23;" width=120 height=80 alt=""/>
              </td>

            </tr>
        </tbody></table>

        
          <hr>
          <table style="width:100%;"><tbody>
              <tr>
              <td style="padding-bottom:20px;width:100%;vertical-align:middle">
                <heading>Publications</heading>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;"><tbody>
      
            <tr>
              <!-- <td style="vertical-align:middle;">
                <a href="papers/scalable-nisr/snisr.pdf"><img src="images/snisr.jpg" width=350></a></td> -->

            <!-- <td style="vertical-align:middle;">
                <video width="350px" poster="images/snisr.jpg" muted="muted" onMouseOver="this.play()" onMouseOut="this.pause()" class="video_class"  loop>
                    <source src="videos/t_video.mp4"  type="video/mp4"></source>
                </video>
            </td> -->
            

            <tr>
                <!-- <td style="vertical-align:middle;">
                    <a href="https://superxjm.github.io/source_files/ReflectiveIBR.pdf"><img src="images/sibr.png" width=350></a>
                </td> -->

                <td style="vertical-align:middle;">
                    <div class="video">
                      <!-- <img src="images/CVPR2023.png" style="padding-top: 23;padding-bottom: 23;" width=400 height=200 alt=""/> -->
                      <video width="260" controls autoplay loop>
                        <source src="videos/CVPR_24M.mp4" type="video/mp4" />
                        <!-- </object> -->
                        </video>
                        <!-- <div class="img2"  onmouseover="come2()"><img src="images/sibr.png" style="padding-top: 23;padding-bottom: 23;" width=350 height=150 alt=""/></div> -->
                        <!-- <a href="此处填写你想要跳转的地址"> -->
                        <!-- <video id="video2"  style="display: none"  src="videos/445.mp4" width=350 height=196 onmouseout="go2()" loop autoplay playsinline></video> -->
                        <!-- </a> -->
                    </div>
                </td>

                <td style="padding:20px;vertical-align:middle">
                    <a></papertitle><b>Unsupervised Domain Adaption with Pixel-level Discriminator for Image-aware Layout Generation</b></papertitle></a>
                  <br>
                  <a href="https://github.com/ChenchenXu218/ChenchenXu218.github.io/"><u>Chenchen Xu*</u></a>,<br>
                  <a href="hhttps://github.com/minzhouGithub/"><u>Min Zhou</u></a>,
                  <a>Tiezheng Ge</a>,
                  <a>Yuning Jiang</a>,
                  <a href="http://www.cad.zju.edu.cn/home/weiweixu/weiweixu_en.htm"><u>Weiwei Xu†</u></a>  
                  <br>
                  <h5><strong>CVPR 2023</strong><h5>
                    (* first author, † corresponding author) <br>
                  <a href=""><strong>project</strong></a> |
                  <a href="https://arxiv.org/pdf/2303.14377.pdf"><strong>paper</strong></a> |
                  <a href="videos/CVPR_24M.mp4"><strong>video</strong></a> |
                  
            
                  <p>Layout is essential for graphic design and poster generation. Recently, applying deep learning models to generate layouts has attracted increasing attention. This paper focuses on using the GAN-based model conditioned on
                    image contents to generate advertising poster graphic layouts, which requires an advertising poster layout dataset
                    with paired product images and graphic layouts. However, the paired images and layouts in the existing dataset
                    are collected by inpainting and annotating posters, respectively. There exists a domain gap between inpainted posters
                    (source domain data) and clean product images (target domain data). Therefore, this paper combines unsupervised
                    domain adaption techniques to design a GAN with a novel
                    pixel-level discriminator (PD), called PDA-GAN, to generate graphic layouts according to image contents. The
                    PD is connected to the shallow level feature map and computes the GAN loss for each input-image pixel. Both quantitative and qualitative evaluations demonstrate that PDAGAN can achieve state-of-the-art performances and generate high-quality image-aware graphic layouts for advertising posters.</p>
                </td>
              </tr> 


            <td style="vertical-align:middle;">
            <div class="video2">
              <img src="images/ijcai2022.png" width=400 height=200 alt=""/>
            </div>
            </td>
    

              <td style="padding-left:20px; vertical-align:middle;">
                <a><papertitle><b>Composition-aware Graphic Layout GAN for Visual-textual Presentation Designs</b></papertitle></a>
                <br>
                <a href="hhttps://github.com/minzhouGithub/"><u>Min Zhou*</u></a>,
                <a href="https://github.com/ChenchenXu218/ChenchenXu218.github.io/"><u>Chenchen Xu*</u></a>, <br>
                <a>Ye Ma</a>,
                <a>Tiezheng Ge</a>,
                <a>Yuning Jiang</a>,
                <a href="http://www.cad.zju.edu.cn/home/weiweixu/weiweixu_en.htm"><u>Weiwei Xu†</u></a>
                <br>
          <h5><strong>IJCAI 2022</strong><h5> 
             (* joint first author, † corresponding author) <br>
                <a href=""><strong>project</strong></a> |
                <a href="https://www.ijcai.org/proceedings/2022/0692.pdf"><strong>paper</strong></a> |
                <a href=""><strong>video</strong></a> |
                <p>In this paper, we study the graphic layout generation problem of producing high-quality visualtextual presentation designs for given images. We note that image compositions, which contain not only global semantics but also spatial information, would largely affect layout results. Hence, we propose a deep generative model, dubbed as composition-aware graphic layout GAN (CGLGAN), to synthesize layouts based on the global and spatial visual contents of input images. To obtain training images from images that already contain manually designed graphic layout data, previous work suggests masking design elements (e.g., texts and embellishments) as model inputs, which inevitably leaves hint of the ground truth. We study the misalignment between the training inputs (with hint masks) and test inputs (without masks), and design a novel domain alignment module (DAM) to narrow this gap. For training, we built a large-scale layout dataset which consists of 60,548 advertising posters with annotated layout information. To evaluate the generated layouts, we propose three novel metrics according to aesthetic intuitions. Through both quantitative and qualitative evaluations, we demonstrate that the proposed model can synthesize highquality graphic layouts according to image compositions</p>
              </td>
            </tr> 


            <td style="vertical-align:middle;">
                <div class="video3">
                  <img src="images/fast_mask.png" width=400 height=350 alt=""/>
                </div>
                </td>
        
    
                  <td style="padding-left:20px; vertical-align:middle;">
                    <a><papertitle><b>Fast Vehicle and Pedestrian Detection Using Improved Mask R-CNN</b></papertitle></a>
                    <br>
                    <a href="https://github.com/ChenchenXu218/ChenchenXu218.github.io/"><u>Chenchen Xu*</u></a>, <br>
                    <a href="https://orcid.org/0000-0001-8482-256X"><u>Guili Wang†</u></a>,
                    <a>Songsong Yan</a>,
                    <a>Jianghua Yu</a>,
                    <a>Baojun Zhang</a>,
                    <a>Shu Dai</a>,
                    <a>Yu Li</a>,
                    <a href="https://orcid.org/0000-0001-6952-824X"><u>Lin Xu</u></a>,
                    <br>
              <h5><strong>Mathematical Problems in Engineering</strong><h5> 
                (* first author, † corresponding author) <br>
                    <a href=""><strong>project</strong></a> |
                    <a href="https://www.hindawi.com/journals/mpe/2020/5761414/"><strong>paper</strong></a> |
                    <a href=""><strong>video</strong></a> |
                    <p>This study presents a simple and effective Mask R-CNN algorithm for more rapid detection of vehicles and pedestrians. The method is of practical value for anticollision warning systems in intelligent driving. Deep neural networks with more layers have greater capacity but also have to perform more complicated calculations. To overcome this disadvantage, this study adopts a Resnet-86 network as a backbone that differs from the backbone structure of Resnet-101 in the Mask R-CNN algorithm within practical conditions. The results show that the Resnet-86 network can reduce the operation time and greatly improve accuracy. The detected vehicles and pedestrians are also screened out based on the Microsoft COCO dataset. The new dataset is formed by screening and supplementing COCO dataset, which makes the training of the algorithm more efficient. Perhaps, the most important part of our research is that we propose a new algorithm, Side Fusion FPN. The parameters in the algorithm have not increased, the amount of calculation has increased by less than 0.000001, and the mean average precision (mAP) has increased by 2.00 points. The results show that, compared with the algorithm of Mask R-CNN, our algorithm decreased the weight memory size by 9.43%, improved the training speed by 26.98%, improved the testing speed by 7.94%, decreased the value of loss by 0.26, and increased the value of mAP by 17.53 points.</p>
                  </td>
                </tr> 


                <td style="vertical-align:middle;">
                    <div class="video4">
                      <img src="images/yolact.png" width=400 height=180 alt=""/>
                    </div>
                    </td>
            
        
                      <td style="padding-left:20px; vertical-align:middle;">
                        <a><papertitle><b>Detecting Small Chinese Traffic Signs via Improved YOLOv3 Method</b></papertitle></a>
                        <br>
                        <a>Baojun Zhang*</a>,<br>
                        <a href="https://orcid.org/0000-0001-8482-256X"><u>Guili Wang†</u></a>,
                        <a>Huilan Wang</a>,
                        <a href="https://github.com/ChenchenXu218/ChenchenXu218.github.io/"><u>Chenchen Xu</u></a>,
                        <a>Yu Li</a>,
                        <a href="https://orcid.org/0000-0001-6952-824X"><u>Lin Xu</u></a>,
                        <br>
                  <h5><strong>Mathematical Problems in Engineering</strong><h5> 
                    (* first author, † corresponding author) <br>
                        <a href=""><strong>project</strong></a> |
                        <a href="https://www.hindawi.com/journals/mpe/2021/8826593/"><strong>paper</strong></a> |
                        <a href=""><strong>video</strong></a> |
                        <p>Long-distance detection of traffic signs provides drivers with more reaction time, which is an effective technique to reduce the probability of sudden accidents. It is recognized that the imaging size of far traffic signs is decreasing with distance. Such a fact imposes much challenge on long-distance detection. Aiming to enhance the recognition rate of long-distance small targets, we design a four-scale detection structure based on the three-scale detection structure of YOLOv3 network. In order to reduce the occlusion effects of similar objects, NMS is replaced by soft-NMS. In addition, the datasets are trained and the K-Means method is used to generate the appropriate anchor boxes, so as to speed up the network computing. By using these methods, better experimental results for the recognition of long-distance traffic signs have been obtained. The recognition rate is 43.8 frames per second (FPS), and the recognition accuracy is improved to 98.8%, which is much better than the original YOLOv3.</p>
                      </td>
                    </tr> 
      

  
        <!-- </tbody></table></table> -->
        

        <footer class="footer">
            
        </footer>

        


  
  </body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>

<script src="js/temp.js"></script>
<script>
    myVid=document.getElementById("video");
    myVid.muted=true;
    function come() {
        $("#video").show();
        $(".img").hide();
        // $("#video").load();//执行一次加载一次，从头开始播放
        // $("#video").currentTime = 0;
        $("#video")[0].play();
    }
    function go() {
        $(".img").show();
        $("#video").hide();
        $("#video")[0].pause();
        // $("#video").get(0).currentTime = 0;
    }
</script>
<script>
    myVid=document.getElementById("video2");
    myVid.muted=true;
    function come2() {
        $("#video2").show();
        $(".img2").hide();
        // $("#video").load();//执行一次加载一次，从头开始播放
        $("#video2")[0].play();
    }
    function go2() {
        $(".img2").show();
        $("#video2").hide();
        $("#video2")[0].pause();
    }
</script>
<script>
    myVid=document.getElementById("video3");
    myVid.muted=true;
    function come2() {
        $("#video3").show();
        $(".img3").hide();
        // $("#video").load();//执行一次加载一次，从头开始播放
        $("#video3")[0].play();
    }
    function go2() {
        $(".img3").show();
        $("#video3").hide();
        $("#video3")[0].pause();
    }
</script>
<script>
    myVid=document.getElementById("video4");
    myVid.muted=true;
    function come2() {
        $("#video4").show();
        $(".img4").hide();
        // $("#video").load();//执行一次加载一次，从头开始播放
        $("#video4")[0].play();
    }
    function go2() {
        $(".img4").show();
        $("#video4").hide();
        $("#video4")[0].pause();
    }
</script>
